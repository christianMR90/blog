---
title: "MAPS Crowdsourced Open Science Project: Data Exploration"
author: "Corey Pembleton"
date: '2019-04-27'
output: html_document
slug: crowdsourced-open-science-project-data-exploration
categories: [ "R", "dataexploration"]
tags: ["data cleaning", "statistics", "naniar"]
thumbnailImagePosition: "left"
draft: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
library(kableExtra)
```

In my previous post I explain the goals of this research project, and now that I have access to the data, it's time to dig in and have a look at the data used for the analysis, how gaps in the data were imputed, and what other approaches could be used.

### Synthetic Data 

One of the contributors of the MAPS study, [Robert Arbon](https://twitter.com/BertieArbon), created a synthesized version of the ALSPAC dataset using the R ```SynthPop``` package to account for privacy concerns in the data, and gave a detailed documentation on how the synthetic dataset (the one which will be used for this present MAPS study) was created.

### Missing Data & Imputations

the large amounts of missing data (only 12.7% of participants had complete data on outcomes, exposure, and covariates). They describe the method as being similar to the Multiple Imputation by Chained Equations (MICE) method (described in detail [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)). 

The flow of how the authors arrived at only 12.7% of participants having completed the required outcomes/exposures/covariates looks like this:

![](/img/data-flow.png)

In this post, I would like to explore the missing data further to gain an understanding of the data used for the study, how it was imputated, and what implications this could have on future analytic stages.

#### Libraries Used

I will be using ```naniar``` and ```simputation``` along with the usual ```tidyverse``` suspects throughout this post.

```{r, cache=TRUE, error=FALSE, warning=FALSE, message=FALSE}
orig_dat <- read_csv("https://raw.githubusercontent.com/pembletonc/OpenNorth_MAPS/master/maps-synthetic-data.csv") %>% rename(ID = X1)

dat <- orig_dat %>% select(ID, sex, 
                     Secondary_Depression_Diagnosis_17.5 = secd_diag,
                     Primary_Depression_Diagnosis_17.5 = prim_diag,
                     ICD_Depression_Diagnosis_17.5 = has_dep_diag,
                     Depression_Score_17.5 = dep_score,
                     Depression_Band_15.5 = dep_band_15,
                     Panic_Score_17.5 = panic_score,
                     Depressed_Thoughts_17.5 = dep_thoughts,
                     Computer_Weekday_Use_16.5 = comp_week,
                     Computer_Weekend_Use_16.5 = comp_wend,
                     Computer_No_Int_House_14 = comp_house,
                     Computer_Int_Room_14 = comp_int_bed_16,
                     Computer_No_Int_Room_14 = comp_noint_bed_16,
                     Bullying_16.5 = child_bull,
                     Time_Alone_Weekday_16.5 = alon_week,
                     Time_Alone_Weekend_16.5 = alon_wend) 
```


## Find out what's missing
As a first step, I would like to replicate the graphic in the figure above using the synthetic dataset provided by the Bristol research team.

The second figure in the flow diagram features 14,665 "live born offspring (singletons and twins) who had not withdrawn from the study", which is my entire N population:

```{r}

nrow(orig_dat)

```

The synthetic dataset provided has 13,734 participants, 1,291 less than that used in the study. This difference isn't explicitly explained, but is implied that is the result of removing any rows in common with the original dataset. 

The next figure in the flow diagram, 4,562, is the number of participants who completed the CIS-R at the age of 18. This is either the variables ```dep_score``` or ```dep_thoughts```, which I have renamed ```Depression_Score_17.5``` and ```Depressed_Thoughts_17.5```, respectively.

```{r}
#counter <- function(df) {df %>% filter(!is.na(.)) %>% summarise(count = n())}

#counter(dat$Depression_Score_17.5)

(Dep_Score <- dat %>% filter(!is.na(Depression_Score_17.5)) %>% summarise(count=n()))

dat %>% filter(!is.na(Depressed_Thoughts_17.5)) %>% summarise(count=n())
```

Looks, like the score is the variable that has been used. The third, the exposure variable, is 3,009 participants with "exposure information at age 16", meaning those who had screentime at the age of 16. The authors of the study look at three types of screens: television screens, texting, and computer screens on weekends and weekdays. I have saved these variables for computer use as ```Computer_Weekday_Use_16.5``` & ```Computer_Weekend_Use_16.5```.

```{r}
dat %>% filter(!is.na(Computer_Weekday_Use_16.5) & !is.na(Computer_Weekend_Use_16.5)) %>% summarise(count=n())
```




```{r}
dat %>% naniar::vis_miss()

```

```{r}
dat %>% visdat::vis_dat()

```

```{r}
dat %>% naniar::gg_miss_var(show_pct = TRUE, facet = sex)

```

```{r}
dat %>% naniar::gg_miss_span(ICD_Depression_Diagnosis_17.5, span_every = 1000) + labs(title = "Number of missing ICD Depression Diagnosis")

```


