---
title: "MAPS Crowdsourced Open Science Project: Data Exploration"
author: "Corey Pembleton"
date: '2019-04-27'
output: html_document
slug: crowdsourced-open-science-project-data-exploration
categories: [ "R", "dataexploration"]
tags: ["data cleaning", "statistics", "naniar"]
thumbnailImagePosition: "left"
draft: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
library(kableExtra)
```

In my previous post I explain the goals of this research project, and now that I have access to the data, it's time to dig in and have a look at the data used for the analysis, how gaps in the data were imputed, and what other approaches could be used.

### Synthetic Data 

One of the contributors of the MAPS study, [Robert Arbon](https://twitter.com/BertieArbon), created a synthesized version of the ALSPAC dataset using the R ```SynthPop``` package to account for privacy concerns in the data, and gave a detailed documentation on how the synthetic dataset (the one which will be used for this present MAPS study) was created.

### Missing Data & Imputations

the large amounts of missing data (only 12.7% of participants had complete data on outcomes, exposure, and covariates). They describe the method as being similar to the Multiple Imputation by Chained Equations (MICE) method (described in detail [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)). 

The flow of how the authors arrived at only 12.7% of participants having completed the required outcomes/exposures/covariates looks like this:

![](/img/data-flow.png)

In this post, I would like to explore the missing data further to gain an understanding of the data used for the study, how it was imputated, and what implications this could have on future analytic stages.

#### Libraries Used

I will be using ```naniar``` and ```simputation``` along with the usual ```tidyverse``` suspects throughout this post.

```{r, cache=TRUE, error=FALSE, warning=FALSE, message=FALSE}
orig_dat <- read_csv("https://raw.githubusercontent.com/pembletonc/OpenNorth_MAPS/master/maps-synthetic-data.csv") %>% rename(ID = X1)

dat <- orig_dat %>% select(ID, sex, 
                     Secondary_Depression_Diagnosis_17.5 = secd_diag,
                     Primary_Depression_Diagnosis_17.5 = prim_diag,
                     ICD_Depression_Diagnosis_17.5 = has_dep_diag,
                     Depression_Score_17.5 = dep_score,
                     Depression_Band_15.5 = dep_band_15,
                     Panic_Score_17.5 = panic_score,
                     Depressed_Thoughts_17.5 = dep_thoughts,
                     Computer_Weekday_Use_16.5 = comp_week,
                     Computer_Weekend_Use_16.5 = comp_wend,
                     Computer_No_Int_House_14 = comp_house,
                     Computer_Int_Room_14 = comp_int_bed_16,
                     Computer_No_Int_Room_14 = comp_noint_bed_16,
                     Bullying_16.5 = child_bull,
                     Time_Alone_Weekday_16.5 = alon_week,
                     Time_Alone_Weekend_16.5 = alon_wend, text_week, text_wend,
                     tv_week, tv_wend) 
```


## Find out what's missing
As a first step, I would like to replicate the graphic in the figure above using the synthetic dataset provided by the Bristol research team.

The second figure in the flow diagram features 14,665 "live born offspring (singletons and twins) who had not withdrawn from the study", which is my entire N population:

```{r}

nrow(orig_dat)

```

The synthetic dataset provided has 13,734 participants, 1,291 less than that used in the study. This difference isn't explicitly explained, but is implied that is the result of removing any rows in common with the original dataset. 

The next figure in the flow diagram, 4,562, is the number of participants who completed the CIS-R at the age of 18. This is either the variables ```dep_score``` or ```dep_thoughts```, which I have renamed ```Depression_Score_17.5``` and ```Depressed_Thoughts_17.5```, respectively.

```{r}
#counter <- function(df) {df %>% filter(!is.na(.)) %>% summarise(count = n())}

#counter(dat$Depression_Score_17.5)

(Dep_Score <- dat %>% filter(!is.na(Depression_Score_17.5)) %>% summarise(count=n()))

dat %>% filter(!is.na(Depressed_Thoughts_17.5)) %>% summarise(count=n())
```

Looks, like the score is the variable that has been used. The third, the exposure variable, is 3,009 participants with "exposure information at age 16", meaning those who had screentime at the age of 16. The authors of the study look at three types of screens: television screens, texting, and computer screens on weekends and weekdays. I have saved these variables for computer use as ```Computer_Weekday_Use_16.5``` & ```Computer_Weekend_Use_16.5```, and for tv and texting using ```tv_week```, ```tv_wend```, ```text_week```, and ```text_wend```

```{r}

vars <- c("Computer_Weekday_Use_16.5", "Computer_Weekend_Use_16.5","tv_week","tv_wend",
          "text_week", "text_wend")

dat %>% filter(!is.na(Computer_Weekday_Use_16.5) &
                 !is.na(Computer_Weekend_Use_16.5) &
                 !is.na(tv_week) & !is.na(tv_wend) & !is.na(text_week), !is.na(text_wend)) %>%  summarise(count=n()) 

```

Which gives a count of 4,778, however, when including only those who have completed the CIS-R at the age of 18, the figure is `r dat %>% filter(!is.na(Depression_Score_17.5) & !is.na(Computer_Weekday_Use_16.5) &
                 !is.na(Computer_Weekend_Use_16.5) &
                 !is.na(tv_week) & !is.na(tv_wend) & !is.na(text_week), !is.na(text_wend)) %>% summarise(count=n()))`, much closer to the 3,009 in the study.


The last and final determinant of participant relevance to the study, is the inclusion of covariate information, which the authors include in the multitude of models used:


> "Model 2 adjusted for sex, maternal age, anxiety/depression at 15 years, maternal anxiety and depression, maternal education, and parental SES. Model 3 also included IQ, parental conflict, presence of the child’s father, number of people living in the child’s home, bullying, and family TV use in early life. Each of the sub-models of model 4 additionally adjusted for time spent engaging in one other activity on weekdays or weekends (time alone [model 4a], on transport [model 4b], playing outdoors in summer [model 4c], playing outdoors in winter [model 4d], playing with others [model 4e], drawing, making or constructing things [model 4f], exercising [model 4g], completing school or college work [model 4h], reading [model 4i], playing musical instruments [model 4j], talking on a mobile phone [model 4k] and talking on a landline phone [model 4l])."

When including this laundry list of covariates, the "final" figure of available data is 1,586 (Model 3) participants:

```{r}

vars_model3 <- c("dep_score", "text_wend", "text_week", "tv_wend", "tv_week",
                 "comp_week", "comp_wend", "sex","anx_band_15",
                 "mat_anx_0m","mat_anx_1","mat_anx_18m", "mat_anx_8m",
                 "mat_dep", "mat_edu", "mat_ses", "iq", "pat_pres", 
                 "pat_pres_10", "pat_pres_8", "num_home", "phys_cruel", "emot_crul",
                 "child_bull","fam_tv_aft", "fam_tv_eve", "fam_tv_morn")


orig_dat %>% select(one_of(vars_model3)) %>% filter_all(all_vars(!is.na(.))) %>% summarise(count = n())

```

The last variable list in model 3 is the largest gap between the synthetic dataset and the original participant figures, before proceeding with the analysis I'll be looking into whether or not I missed something with the MAPS research team. In the meantime, now that I now what's there and what isn't there, it would be interesting to explore further what's missing before imputing the dataset.

### Exploring the missings





```{r}
dat %>% naniar::vis_miss()

```

```{r}
dat %>% visdat::vis_dat()

```

```{r}
dat %>% naniar::gg_miss_var(show_pct = TRUE, facet = sex)

```

```{r}
dat %>% naniar::gg_miss_span(ICD_Depression_Diagnosis_17.5, span_every = 1000) + labs(title = "Number of missing ICD Depression Diagnosis")

```


